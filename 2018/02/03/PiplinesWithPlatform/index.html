
<!DOCTYPE html>
<html lang="zh-CN">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="theme-color" content="#202020" />
  <meta http-equiv="x-ua-compatible" content="ie=edge">

  
  <meta name="keywords" content="Data,机器学习," />
  

  
  <meta name="description" content="Building Complex Data Pipelines with Unified Analytics Platform" />
  

  <link rel="icon" type="image/x-icon" href="/img/logo.png">
  <title>
    Building Complex Data Pipelines with Unified Analytics Platform [ 仁梓小舍 ]
  </title>
  
  <!-- stylesheets list from config.yml -->
  
  <link rel="stylesheet" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css">
  
  <link rel="stylesheet" href="/css/microb.css">
  
  
<link rel="stylesheet" href="/css/prism.css" type="text/css"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="home-menu pure-menu pure-menu-horizontal pure-menu-fixed">
  <ul class="pure-menu-list float-r clearfix">
    
    <li class="pure-menu-item toc-menu">
      <a id="menu-main-post" class="pure-menu-link" href="javascript:;">
        <img class="menu-icon" src="/img/logo.png" alt="MENU">
      </a>
    </li>
    
  </ul>
  <a class="pure-menu-heading" href="/">
    <h1 class="title">
      仁梓小舍
    </h1>
    <!-- <span>null</span> -->
  </a>
  <!-- 
  <img class="logo" id="logo" src="/logo.png" alt="logo">
   -->
</nav>

  <div class="container" id="content-outer">
    <div class="inner" id="content-inner">
      <article class="post" id="post">
  <header class="post-header text-center">
    <h1 class="title">
      Building Complex Data Pipelines with Unified Analytics Platform
    </h1>
    
    <time class="time" datetime="2018-02-03T07:07:27.000Z">
      2018-02-03
    </time>
    
    <hr>
  </header>
  <div class="post-content">
    <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在<code>Quora</code>上,大数据从业者经常会提出以下重复的问题：<a href="https://www.quora.com/What-is-the-use-of-Data-engineer-in-Apache-Spark" target="_blank" rel="external">什么是数据工程（<code>Data Engineering</code>）？</a> <a href="https://www.quora.com/How-can-I-become-a-data-scientist-1" target="_blank" rel="external">如何成为一名数据科学家（<code>Data Scientist</code>）？</a> <a href="https://www.quora.com/What-is-a-data-analyst-1" target="_blank" rel="external">什么是数据分析师（<code>Data Analyst</code>）？</a></p>
<p>除了理解上述三种职业及其职能之外，更重要的问题是：如何去促进这三种不同的职业、职能和其诉求之间的协作？或者怎样去帮助他们采用统一的平台来代替一次性定制解决方案？</p>
<p>现在他们确实可以使用统一的平台进行协作了。上个月，我们发布了<a href="https://databricks.com/product/databricks" target="_blank" rel="external">统一数据块平台</a>。针对促进数据工程师，数据科学家和数据分析师之间的协作，其软件工件 <code>Databricks Workspace</code> 和 <code>Notebook Workflows</code> 实现了这令人梦寐以求的协作。</p>
<p>在这篇博文中，我们将探讨每种角色以下三种赋能</p>
<ul>
<li>使用 <a href="https://databricks.com/blog/2016/08/30/notebook-workflows-the-easiest-way-to-implement-apache-spark-pipelines.html" target="_blank" rel="external">Notebook Workflows</a>来协作和构建复杂的 <code>Apache Spark</code> 的数据管道</li>
<li>将独立和幂等的笔记本作为 <em>单一执行单元</em> 进行编排</li>
<li>无需定制一次性或独特的解决方案。</li>
</ul>
<h2 id="亚马逊公共产品评级"><a href="#亚马逊公共产品评级" class="headerlink" title="亚马逊公共产品评级"></a>亚马逊公共产品评级</h2><p>首先，我们来看看数据场景。我们的数据场景视为<a href="https://snap.stanford.edu/data/web-Amazon.html" target="_blank" rel="external">亚马逊公共产品评级</a>的语料库，其中每个角色都希望以可被理解的形式执行各自的任务。</p>
<p><img src="https://ask.qcloudimg.com/draft/1206541/fvkvz3fn52.png" alt=""></p>
<p>这个数据集是产品评论的不同数据文件的集合，对于任何数据科学家或数据分析师都很重要。例如，数据分析师的目的可能是探索数据以检查其存在哪种评级，产品类别或品牌。相比之下，数据科学家的目的可能想要训练一个机器学习模型，有利于定期对用户评论中某些关键词（如“好”、“回归”或“糟糕”）进行评级。</p>
<p>但是，如果没有事先将数据转化为可供每个角色使用的格式，那么既不能方便数据分析员对其进行探索，也不便于数据科学家进行模型训练。这就是数据工程师引入公式的原因：她负责通过创建数据管道将原始数据转换为可用数据。（我们所说的<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650366/3601578643761083/latest.html" target="_blank" rel="external"><em>ExamplesIngestingData</em></a>笔记本工具是数据工程师将摄取到的公共数据集嵌入 <code>Databricks</code>平台的过程。）</p>
<p>接下来，我们将检查我们的第一个数据流水线，第一个笔记本工具<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650486/3601578643761083/latest.html" target="_blank" rel="external"><em>TrainModel</em></a>，其可以提供浏览与每个角色相关的任务的功能。</p>
<h2 id="Apache-Spark作业的数据流水线"><a href="#Apache-Spark作业的数据流水线" class="headerlink" title="Apache Spark作业的数据流水线"></a>Apache Spark作业的数据流水线</h2><p><img src="https://ask.qcloudimg.com/draft/1206541/0e1ngh0tou.jpg" alt=""></p>
<h3 id="探索数据"><a href="#探索数据" class="headerlink" title="探索数据"></a>探索数据</h3><p>为了简单起见，我们不会涉及将原始数据转换为以供 <code>JSON</code> 文件摄取的 <code>Python</code> 代码 - 代码位于此<a href="https://snap.stanford.edu/data/web-Amazon.html" target="_blank" rel="external">链接</a>。相反，我们将专注于我们的数据管道笔记本工具，<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650486/3601578643761083/latest.html" target="_blank" rel="external"><em>TrainModel</em></a>，帮助数据科学家和数据分析师进行协作。</p>
<p>我们的数据工程师一旦将产品评审的语料摄入到 <code>Parquet</code> (注：Parquet是面向分析型业务的列式存储格式)文件中, 通过 <code>Parquet</code> 创建一个可视化的 <code>Amazon</code> 外部表, 从该外部表中创建一个临时视图来浏览表的部分, 数据分析员和数据科学家都可以在这个 <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650486/3601578643761083/latest.html" target="_blank" rel="external"><em>TrainModel</em></a> 的笔记本工具中合作工作。</p>
<p><img src="https://databricks.com/wp-content/uploads/2017/10/image2.png" alt=""></p>
<p><img src="https://databricks.com/wp-content/uploads/2017/10/image4.png" alt=""></p>
<p>数据分析师可以利用 <code>SQL</code> 查询，而不是用数据工程师或数据科学家比较熟悉的 <code>Python</code> 代码进行查询。这里的要点是，笔记本的语言类型（无论是 <code>Scala</code> ，<code>Python</code>，<code>R</code>还是 <code>SQL</code>）的优势是次要的，而以熟悉的语言（即 <code>SQL</code>）表达查询并与其他人合作的能力是最重要的。</p>
<p>现在，每个角色都有可理解的数据，作为临时表 <code>tmp_table</code> 业务问题和数据可视化; 她可以查询此表，例如，以下问题：</p>
<h3 id="数据是什么样的？"><a href="#数据是什么样的？" class="headerlink" title="数据是什么样的？"></a>数据是什么样的？</h3><p><img src="https://databricks.com/wp-content/uploads/2017/10/image7.png" alt=""></p>
<h3 id="有多少个不同的品牌？"><a href="#有多少个不同的品牌？" class="headerlink" title="有多少个不同的品牌？"></a>有多少个不同的品牌？</h3><p><img src="https://databricks.com/wp-content/uploads/2017/10/image10.png" alt=""></p>
<h3 id="如何保证公平地进行品牌评分？"><a href="#如何保证公平地进行品牌评分？" class="headerlink" title="如何保证公平地进行品牌评分？"></a>如何保证公平地进行品牌评分？</h3><p><img src="https://databricks.com/wp-content/uploads/2017/10/image8.png" alt=""></p>
<p>她的初步分析令人很满意，她可能会帮助一位数据科学家，进而设计一个机器学习模型，使他们能够定期预测用户评论的评分。随着用户在亚马逊网站上每天甚至每周购买和评价产品，机器学习模型可以在生产中定期进行训练新的数据。</p>
<h2 id="培训机器学习模型"><a href="#培训机器学习模型" class="headerlink" title="培训机器学习模型"></a>培训机器学习模型</h2><p><code>Apache Spark</code> 的<a href="https://spark.apache.org/mllib/" target="_blank" rel="external">机器学习库MLlib</a>包含许多用于分类，回归，聚类和协作过滤的算法。在高层次上，<em>spark.ml</em> 包为特征化，流水线，数学实用程序和持久性提供了工具，技术和 <code>API</code> 。</p>
<p>当涉及基于特定关键字的好（1）或差（0）结果的二元预测时，适合于该分类的最佳模型是<a href="https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression" target="_blank" rel="external">Logistic回归模型</a>，这是一种预测有利结果概率的特殊情况的<a href="https://en.wikipedia.org/wiki/Generalized_linear_model" target="_blank" rel="external">广义线性模型</a>。</p>
<p>在我们的案例中，我们希望用一些有利的关键词来预测评论的评分结果。我们不仅要使用 <code>MLlib</code> 提供的逻辑回归模型族的二项逻辑回归，还要使用<a href="https://databricks.com/blog/2015/01/07/ml-pipelines-a-new-high-level-api-for-mllib.html" target="_blank" rel="external"><em>spark.ml</em>管道</a>及其<a href="https://spark.apache.org/docs/2.0.0-preview/ml-guide.html" target="_blank" rel="external">变形和估计器</a>。</p>
<h2 id="创建机器学习管道"><a href="#创建机器学习管道" class="headerlink" title="创建机器学习管道"></a>创建机器学习管道</h2><p>Python代码片段如何用变换器和估计器创建管道。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">from pyspark.ml import *</div><div class="line">from pyspark.ml.feature import *</div><div class="line">from pyspark.ml.feature import Bucketizer</div><div class="line">from pyspark.ml.classification import *</div><div class="line">from pyspark.ml.tuning import *</div><div class="line">from pyspark.ml.evaluation import *</div><div class="line">from pyspark.ml.regression import *</div><div class="line">#</div><div class="line"># Bucketizer transforms a column of continuous features to a column of feature buckets, where the buckets are specified by users.</div><div class="line"># It takes the common parameters inputCol and outputCol, as well as the splits for bucketization.</div><div class="line"># Feature values greater than the threshold are bucketized to 1.0; values equal to or less than the threshold</div><div class="line"># are binarized to 0.0. Both Vector and Double types are supported for inputCol.</div><div class="line"># We will use rating as our input, and the output label will have value of 1 if &gt; 4.5</div><div class="line">#</div><div class="line"># For this model we will use two feature transformer extractors: Bucketizer and Tokenizer</div><div class="line">#</div><div class="line">splits = [-float(&quot;inf&quot;), 4.5, float(&quot;inf&quot;)]</div><div class="line">tok = Tokenizer(inputCol = &quot;review&quot;, outputCol = &quot;words&quot;)</div><div class="line">bucket = Bucketizer(splits=splits, inputCol = &quot;rating&quot;, outputCol = &quot;label&quot;)</div><div class="line">#</div><div class="line"># use HashingTF feature extractor, with its input as &quot;words&quot;</div><div class="line">#</div><div class="line">hashTF = HashingTF(inputCol = tok.getOutputCol(), numFeatures = 10000, outputCol = &quot;features&quot;)</div><div class="line">#</div><div class="line"># create a model instance with some parameters</div><div class="line">#</div><div class="line">lr = LogisticRegression(maxIter = 10, regParam = 0.0001, elasticNetParam = 1.0)</div><div class="line">#</div><div class="line"># Create the stages pipeline with all the feature transformers to create an Estimator</div><div class="line">#</div><div class="line">pipeline = Pipeline(stages = [bucket, tok, hashTF, lr])</div></pre></td></tr></table></figure>
<h3 id="创建训练方式和测试数据"><a href="#创建训练方式和测试数据" class="headerlink" title="创建训练方式和测试数据"></a>创建训练方式和测试数据</h3><p>接下来，我们使用我们的训练数据来拟合模型，最后用我们的测试框架 <code>perdictions</code> 进行预测和建立标签。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># Create our model estimator</div><div class="line">#</div><div class="line">model = pipeline.fit(trainingData)</div><div class="line"></div><div class="line">#score the model with test data</div><div class="line">predictions = model.transform(testData)</div><div class="line">#convert dataframe into a table so we can easily query it using SQL</div><div class="line">predictions.createOrReplaceTempView(&apos;tmp_predictions&apos;)</div></pre></td></tr></table></figure>
<p><img src="https://databricks.com/wp-content/uploads/2017/10/image9.png" alt=""></p>
<p>正如您可能注意到, 通过上述的 <code>predictions</code> 函数查询后放入 <code>DataFrame</code> 保存为一个临时表, 在我们的测试数据的评论中出现的单词 <code>return</code> 的结果在价值0的 <code>Prediction</code> 和 <code>Label</code> 和低评级的预期。</p>
<p>对于评估模型的结果感到满意，数据科学家可以将模型保存为与其他数据科学家共享，甚至进一步评估或与数据工程师共享，以便在生产中部署。</p>
<p>这伴随着实时模型。</p>
<h2 id="实时模式"><a href="#实时模式" class="headerlink" title="实时模式"></a>实时模式</h2><p>考虑一下数据科学家生成<code>ML</code>模型，并想要测试和迭代它，将其部署到生产中以进行实时预测服务或与另一位数据科学家共享以进行验证用例和场景。你怎么做到的？</p>
<p><a href="https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html" target="_blank" rel="external">坚持和序列化ML管道</a>是导出 <code>MLlib</code> 模型的一种方法。另一种方法是使用<a href="https://go.databricks.com/productionize-apache-spark-mllib-model-scoring" target="_blank" rel="external">Databricks dbml-local库</a>，这是实时服务的低延迟需求下的首选方式。<strong>一个重要的警告：</strong> 对于服务模型的低延迟要求，我们建议并倡导使用 <code>dbml-local</code>。然而对于这个例子，因为延迟不是定期产品评论的问题或要求，所以我们使用 <code>MLlib</code> 管线 <code>API</code> 来导出和导入模型。</p>
<p>尽管 <code>dbml-local</code> 是我们首选的导出和导入模型的方式，但是由于很多原因，两种持久性机制都很重要。首先，它很容易和语言无关 - 模型导出为 <code>JSON</code>。其次，它可以从一个用 <code>Python</code> 编写的笔记本中导出，并导入（加载）到另一个用 <code>Scala</code> 写成的笔记本中，持久化和序列化一个 <code>ML</code> 管道，交换格式是独立于语言的。第三，序列化和坚持流水线封装了所有的功能，而不仅仅是模型。最后，如果您希望通过结构化流式传输来实时预测您的模型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.write().overwrite().save(&quot;/mnt/jules/amazon-model&quot;)</div></pre></td></tr></table></figure>
<p>在我们<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650486/3601578643761083/latest.html" target="_blank" rel="external"><em>TrainModel</em></a>笔记本工具，我们出口我们的模型，以便它可以由其他笔记本工具进口，<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650404/3601578643761083/latest.html" target="_blank" rel="external"><em>ServeModel</em></a>，在我们的笔记本工具链接的下游的工作流程（见下文）。</p>
<p>在下一节中，我们将讨论我们的第二个管道工具<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650526/3601578643761083/latest.html" target="_blank" rel="external"><em>CreateStream</em></a>。</p>
<h2 id="创建流"><a href="#创建流" class="headerlink" title="创建流"></a>创建流</h2><p>考虑一下这种情况：我们可以访问产品评论的实时流，并且使用我们训练有素的模型，我们希望对我们的模型进行评分。数据工程师可以通过两种方式提供这种实时数据：一种是通过 <code>Kafka</code> 或 <code>Kinesis</code>，当用户在 <code>Amazon</code> 网站上评价产品时; 另一个通过插入到表中的新条目（不属于训练集），将它们转换成 <code>S3</code> 上的 <code>JSON</code> 文件。事实上，这只是起作用，因为结构化流式 <code>API</code>以相同的方式读取数据，无论您的数据源是 <code>Blob</code> ，<code>S3</code> 中的文件，还是来自 <code>Kinesis</code> 或 <code>Kafka</code> 的流。我们选择了<code>S3</code>分布式队列来实现低成本和低延迟。</p>
<p><img src="https://ask.qcloudimg.com/draft/1206541/7s1nndfhvx.jpg" alt=""></p>
<p>在我们的例子中，数据工程师可以简单地从我们的表中提取最近的条目，在 <code>Parquet</code> 文件上建立。这个短的管道包含三个 <code>Spark</code> 作业：</p>
<ol>
<li>从 <code>Amazon</code> 表中查询新的产品数据</li>
<li>转换生成的 <code>DataFrame</code></li>
<li>将我们的数据框存储为 <code>S3</code> 上的 <code>JSON</code> 文件</li>
</ol>
<p>为了模拟流，我们可以将每个文件作为 <code>JSON</code> 数据的集合提供的流数据来评价我们的模型。数据科学家已经培训了一个模型并且数据工程师负责提供一种方法来获取实时数据流，这种情况并不罕见，这种情况持续存在于某个可以轻松读取和评估训练模型的地方。</p>
<p>要了解这是如何实现的，请阅读<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650526/3601578643761083/latest.html" target="_blank" rel="external"><em>CreateStream</em></a>笔记本工具; 它的输出将 <code>JSON</code> 文件作为亚马逊评论的<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650404/3601578643761083/latest.html" target="_blank" rel="external"><em>流向ServeModel</em></a>笔记本工具提供服务，以对我们的持久模型进行评分，这形成了我们的最终管道。</p>
<h2 id="创建服务，导入数据和评分模型"><a href="#创建服务，导入数据和评分模型" class="headerlink" title="创建服务，导入数据和评分模型"></a>创建服务，导入数据和评分模型</h2><p><img src="https://ask.qcloudimg.com/draft/1206541/euk9n18bdm.jpg" alt=""></p>
<p>考虑最后的情况：我们现在可以访问新产品评论的实时流（或接近实时流），并且可以访问我们的训练有素的模型，这个模型在我们的 <code>S3</code> 存储桶中保存。数据科学家可以使用这些资产。</p>
<p>让我们看看如何。在我们的例子中，数据科学家可以简单地创建四个 <code>Spark</code> 作业的短管道：</p>
<ol>
<li>从数据存储加载模型</li>
<li>作为 <code>DataFrame</code> 输入流读取 <code>JSON</code> 文件</li>
<li>用输入流转换模型</li>
<li>查询预测</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">···scala</div><div class="line"><span class="comment">// load the model from S3 path</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">PipelineModel</span></div><div class="line"><span class="keyword">val</span> model = <span class="type">PipelineModel</span>.load(model_path)</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</div><div class="line"><span class="comment">// define a the JSON schema for our stream of JSON files</span></div><div class="line"><span class="keyword">val</span> streamSchema = <span class="keyword">new</span> <span class="type">StructType</span>()</div><div class="line">  .add(<span class="type">StructField</span>(<span class="string">"rating"</span>,<span class="type">DoubleType</span>,<span class="literal">true</span>))</div><div class="line">  .add(<span class="type">StructField</span>(<span class="string">"review"</span>,<span class="type">StringType</span>,<span class="literal">true</span>))</div><div class="line">  .add(<span class="type">StructField</span>(<span class="string">"time"</span>,<span class="type">LongType</span>,<span class="literal">true</span>))</div><div class="line">  .add(<span class="type">StructField</span>(<span class="string">"title"</span>,<span class="type">StringType</span>,<span class="literal">true</span>))</div><div class="line">  .add(<span class="type">StructField</span>(<span class="string">"user"</span>,<span class="type">StringType</span>,<span class="literal">true</span>))</div><div class="line"></div><div class="line"><span class="comment">//read streams</span></div><div class="line">spark.conf.set(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"4"</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> inputStream = spark</div><div class="line">  .readStream</div><div class="line">  .schema(streamSchema)</div><div class="line">  .option(<span class="string">"maxFilesPerTrigger"</span>, <span class="number">1</span>)</div><div class="line">  .json(stream_path)</div><div class="line"></div><div class="line"><span class="comment">// transform with the new data in the stream</span></div><div class="line"><span class="keyword">val</span> scoredStream = model.transform(inputStream)</div><div class="line"></div><div class="line"><span class="comment">// and use the stream query for predictions</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> queryStream = scoredStream.writeStream</div><div class="line">  .format(<span class="string">"memory"</span>)</div><div class="line">  .queryName(<span class="string">"streamPrediction"</span>)</div><div class="line">  .start()</div><div class="line"><span class="comment">// query the transformed DataFrame with new predictions</span></div></pre></td></tr></table></figure>
<p>由于所有的特征都被封装在持久化的模型中，所以我们只需要从磁盘加载这个序列化的模型，并使用它来服务和评分我们的新数据。此外，请注意，我们在笔记本<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650486/3601578643761083/latest.html" target="_blank" rel="external"><em>TrainModel中</em></a>创建了这个模型，它是用 <code>Python</code> 编写的，我们在一个 <code>Scala</code> 笔记本中加载。这表明，无论每个角色用于创建笔记本的语言如何，他们都可以共享 <code>Apache Spark</code> 中支持的语言的持久化模型。</p>
<h2 id="Databricks-Notebook工作流程编排"><a href="#Databricks-Notebook工作流程编排" class="headerlink" title="Databricks Notebook工作流程编排"></a>Databricks Notebook工作流程编排</h2><p>协作和协调的核心是<a href="https://docs.databricks.com/user-guide/notebooks/notebook-workflows.html" target="_blank" rel="external">Notebook Workflows的API</a>。使用这些API，数据工程师可以将所有上述管道作为 <em>单个执行单元</em> 串在一起。</p>
<p><img src="https://databricks.com/wp-content/uploads/2017/10/Webp.net-gifmaker-1.gif" alt=""></p>
<p>实现这一目标的一个途径是在笔记本电脑中分享输入和输出。也就是说，笔记本的输出和退出状态将作为流入下一个笔记本的输入。<a href="https://docs.databricks.com/user-guide/notebooks/widgets.html" target="_blank" rel="external">Notebook Widgets</a>允许参数化笔记本输入，而笔记本的退出状态可以将参数传递给流中的下一个参数。</p>
<p>在我们的示例中，<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650520/3601578643761083/latest.html" target="_blank" rel="external"><em>RunNotebooks</em></a>使用参数化参数调用流中的每个笔记本。它将编排另外三个笔记本，每个笔记本都执行自己的数据管道，在其中创建自己的 <code>Spark</code> 作业，最后发出一个 <code>JSON</code> 文档作为退出状态。这个 <code>JSON</code> 文档然后作为管道中后续笔记本的输入参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"># do the usual import packages</div><div class="line">import json</div><div class="line">import sys</div><div class="line">#</div><div class="line">#Run the notebook and get the path to the table</div><div class="line">#fetch the return value from the callee 001_TrainModel</div><div class="line">returned_json = json.loads(dbutils.notebook.run(&quot;001_TrainModel&quot;, 3600, &#123;&#125;))</div><div class="line">if returned_json[&apos;status&apos;] == &apos;OK&apos;:</div><div class="line">  model_path = returned_json[&apos;model_path&apos;]</div><div class="line">  try:</div><div class="line">    #Create a Stream from the table</div><div class="line">    #Fetch the return value from the callee 002_CreateStream</div><div class="line">    returned_json = json.loads(dbutils.notebook.run(&quot;002_CreateStream&quot;, 3600, &#123;&#125;))</div><div class="line">    if returned_json [&apos;status&apos;] == &apos;OK&apos;:</div><div class="line">      stream_path = returned_json[&apos;stream_path&apos;]</div><div class="line">      map = &#123;&quot;model_path&quot;: model_path, &quot;stream_path&quot;: stream_path &#125;</div><div class="line">          #fetch the return value from the callee 003_ServeModelToStructuredStream</div><div class="line">      result = dbutils.notebook.run(&quot;003_ServeModelToStreaming&quot;, 7200, map)</div><div class="line">      print result</div><div class="line">    else:</div><div class="line">      raise &quot;Notebook to create stream failed!&quot;</div><div class="line">  except:</div><div class="line">    print(&quot;Unexpected error:&quot;, sys.exc_info()[0])</div><div class="line">    raise</div><div class="line">else:</div><div class="line">  print &quot;Something went wrong &quot; + returned_json[&apos;message&apos;]</div></pre></td></tr></table></figure>
<p>最后，不仅可以运行这个特定的笔记本执行一个简单的任务，而且你也可以使用<a href="https://docs.databricks.com/api/latest/jobs.html" target="_blank" rel="external">Job Scheduler</a>来安排流程。</p>
<p><img src="https://databricks.com/wp-content/uploads/2017/09/Screen-Shot-2017-09-01-at-11.37.50-AM.png" alt=""></p>
<h2 id="下一步是什么"><a href="#下一步是什么" class="headerlink" title="下一步是什么"></a>下一步是什么</h2><p>为了真正感受<a href="https://databricks.com/product/databricks" target="_blank" rel="external">统一分析平台中</a>三个人物角色之间的端到端协作，请在<a href="https://databricks.com/try-databricks" target="_blank" rel="external">Databricks平台</a>上试用这五款笔记本工具。</p>
<ol>
<li>由数据工程师创建的 <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650520/3601578643761083/latest.html" target="_blank" rel="external"><em>RunNotebooks</em></a></li>
<li>由数据工程师，数据分析师和数据科学家创建的 <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650486/3601578643761083/latest.html" target="_blank" rel="external"><em>TrainModel</em></a></li>
<li>由数据工程师创建的<a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650526/3601578643761083/latest.html" target="_blank" rel="external"> <em>CreateStream</em></a></li>
<li>由数据科学家和数据工程师创建的 <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650404/3601578643761083/latest.html" target="_blank" rel="external"><em>ServeModel</em></a></li>
<li>为数据工程师提供的样品笔记本 <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8599738367597028/327074921650366/3601578643761083/latest.html" target="_blank" rel="external"><em>ExamplesIngestingData</em></a>，</li>
</ol>
<p>总之，我们证明了大数据从业者可以在 <code>Databricks</code> 的<a href="https://databricks.com/product/databricks" target="_blank" rel="external">统一分析平台中</a>一起工作，创建笔记本，探索数据，训练模型，导出模型，并根据新的实时数据评估他们的训练模型。当复杂的数据管道时，当由不同的人物角色构建的无数笔记本可以作为一个单一且连续的执行单元来执行时，它们一起变得高效。通过 <code>Notebook Workflows API</code>，我们展示了一个统一的体验，而不是定制的一次性解决方案。这些好处是有承诺的。</p>
<h2 id="阅读更多"><a href="#阅读更多" class="headerlink" title="阅读更多"></a>阅读更多</h2><p>要了解Github中的笔记本工作流和Widgets以及笔记本集成，请阅读以下内容：</p>
<ul>
<li><a href="https://databricks.com/blog/2016/08/30/notebook-workflows-the-easiest-way-to-implement-apache-spark-pipelines.html" target="_blank" rel="external">笔记本工作流程：实现Apache Spark管道的最简单的方法</a></li>
<li><a href="https://docs.databricks.com/user-guide/notebooks/notebook-workflows.html" target="_blank" rel="external">笔记本工作流程</a></li>
<li><a href="https://docs.databricks.com/user-guide/notebooks/widgets.html" target="_blank" rel="external">笔记本小工具</a></li>
<li><a href="https://docs.databricks.com/user-guide/notebooks/github-version-control.html#" target="_blank" rel="external">笔记本Github集成工具</a></li>
</ul>
<p><img src="https://databricks.com/wp-content/themes/databricks/assets/images/blog/Databricks-logo-bug.png?v=2.2.39" alt=""></p>
<p> 免费试用 <code>Databricks</code>， <a href="https://databricks.com/try-databricks" target="_blank" rel="external">从今天开始</a></p>


    <!-- 显示photos -->
    

  </div>
  <div class="post-tags">
    <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Data/">Data</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>
  </div>
</article>
  
    <div class="post-nav">
      <div class="post-nav-item post-nav-next">
        
          <a href="/2018/01/21/manage/" rel="next" title="管理学·书单">
            <span>〈 </span> 管理学·书单
          </a>
        
      </div>
  
      <div class="post-nav-item post-nav-prev">
        
          <a href="/2018/02/07/FeatureSelection/" rel="prev" title="特征选择引言">
            特征选择引言 <span>〉</span>
          </a>
        
      </div>
    </div>
  


  <div id="toc" class="toc-article">
    <strong class="toc-title">目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#介绍"><span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#亚马逊公共产品评级"><span class="toc-text">亚马逊公共产品评级</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Apache-Spark作业的数据流水线"><span class="toc-text">Apache Spark作业的数据流水线</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#探索数据"><span class="toc-text">探索数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据是什么样的？"><span class="toc-text">数据是什么样的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#有多少个不同的品牌？"><span class="toc-text">有多少个不同的品牌？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何保证公平地进行品牌评分？"><span class="toc-text">如何保证公平地进行品牌评分？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#培训机器学习模型"><span class="toc-text">培训机器学习模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建机器学习管道"><span class="toc-text">创建机器学习管道</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建训练方式和测试数据"><span class="toc-text">创建训练方式和测试数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实时模式"><span class="toc-text">实时模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建流"><span class="toc-text">创建流</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#创建服务，导入数据和评分模型"><span class="toc-text">创建服务，导入数据和评分模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Databricks-Notebook工作流程编排"><span class="toc-text">Databricks Notebook工作流程编排</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#下一步是什么"><span class="toc-text">下一步是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#阅读更多"><span class="toc-text">阅读更多</span></a></li></ol>
  </div>


    </div>

    

  </div>

  <footer class="footer text-center">
    <div id="bottom-inner">
      
        <a href="http://hexo.io" target="_blank" class="footer-link">Hexo</a>
      
        <a href="https://github.com/microacup/hexo-theme-micorb" target="_blank" class="footer-link">Theme microb</a>
      
    </div>
  </footer>
  

<!-- <script>
  var titleTime,originTitle=document.title;document.addEventListener("visibilitychange",function(){document.hidden?(document.title="[闷声发大财中...] "+originTitle,clearTimeout(titleTime)):(document.title="[+1s] "+originTitle,titleTime=setTimeout(function(){document.title=originTitle},2e3))});
</script> -->

<script>
  (function (window, document) {
    window.requestAnimationFrame = (function () {
      return window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.oRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        function (callback) {
          setTimeout(callback, 1000 / 60);
        }
    })();

    function init() {
      addMenuEvent();
    }
    init();

    function addMenuEvent() {
      var menu = document.getElementById('menu-main-post');
      if (menu) {
        var toc = document.getElementById('toc');
        menu.title = toc ? '目录' : '回到顶部';
        menu.onclick = function () {
          if (toc) {
            if (toc.style.display == 'block') {
              toc.style.display = 'none';
            } else {
              toc.style.display = 'block';
            }
          } else {
            returnTop();
          }
        };
      }
    }

    var timer = null;

    function returnTop() {
      cancelAnimationFrame(timer);
      timer = requestAnimationFrame(function fn() {
        var oTop = document.body.scrollTop || document.documentElement.scrollTop;
        if (oTop > 0) {
          document.body.scrollTop = document.documentElement.scrollTop = oTop - 50;
          timer = requestAnimationFrame(fn);
        } else {
          cancelAnimationFrame(timer);
        }
      });
    }

  })(window, document);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  



</body>
</html>
